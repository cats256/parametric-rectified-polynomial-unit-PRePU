{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the Parametric Rectified Power Unit (PRePU), which offers superior polynomial approximation capabillity compared to RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom activation function introduces a trainable parameter, scale, and applies ReLU on the inputs, to introduce non-linearity, then softsign to squash the scale to a range between 0 and 2 to keep exploding gradients in check. However, since we are using the power function, aka doing x^a, this introduces unbounded derivative on the activation function and more exploding gradient issues. Techniques for keeping exploding gradients in check include gradient clipping (clipping-by-norm or clipping-by-value), lower learning rate, learning rate warmup, changing weight initialization technique or lower weight intialization variance, batch normalization, using the activation function only for a few layers to learn polynomial features, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(x, scale):\n",
    "    x_relu = torch.relu(x)\n",
    "    power = F.softsign(scale) + 1\n",
    "    return torch.pow(x_relu, power)\n",
    "\n",
    "class CustomActivationLayer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CustomActivationLayer, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return custom_activation(inputs, self.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.29269132e-01,  1.12629181e+00],\n",
       "       [-3.17074082e-01,  1.21743548e+00],\n",
       "       [-6.36190302e-01,  2.42289999e+00],\n",
       "       [-5.19254029e+00,  4.73638021e+02],\n",
       "       [-1.32885853e+00,  1.12392697e+01],\n",
       "       [-5.65352760e-01,  2.04000979e+00],\n",
       "       [ 1.96689229e+00, -1.35986902e+01],\n",
       "       [-1.10441764e+00,  7.20467383e+00],\n",
       "       [-2.80786065e-01,  1.15389732e+00],\n",
       "       [-1.59772708e-01,  1.02334717e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polynomial(x):\n",
    "    return 1 + 0.5 * x ** 1/2 + 2 * x**2 - 3 * x**3\n",
    "\n",
    "num_points = 10000\n",
    "x_values_normal = np.random.normal(0, 2, num_points)\n",
    "\n",
    "y_values_normal = polynomial(x_values_normal)\n",
    "\n",
    "dataset_normal = np.column_stack((x_values_normal, y_values_normal))\n",
    "dataset_normal[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a feed forward nn with ReLU as activation function (241 trainable parameters) and another feed forward nn with PRePU as activation function (273 trainable parameters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FeedFowardReLU                           [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 8]                    16\n",
       "│    └─ReLU: 2-2                         [1, 8]                    --\n",
       "│    └─Linear: 2-3                       [1, 8]                    72\n",
       "│    └─ReLU: 2-4                         [1, 8]                    --\n",
       "│    └─Linear: 2-5                       [1, 8]                    72\n",
       "│    └─ReLU: 2-6                         [1, 8]                    --\n",
       "│    └─Linear: 2-7                       [1, 8]                    72\n",
       "│    └─ReLU: 2-8                         [1, 8]                    --\n",
       "│    └─Linear: 2-9                       [1, 1]                    9\n",
       "==========================================================================================\n",
       "Total params: 241\n",
       "Trainable params: 241\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedFowardReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedFowardReLU, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "model_relu = FeedFowardReLU()\n",
    "summary(model_relu, input_size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FeedFowardPRePU                          [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 8]                    16\n",
       "│    └─CustomActivationLayer: 2-2        [1, 8]                    8\n",
       "│    └─Linear: 2-3                       [1, 8]                    72\n",
       "│    └─CustomActivationLayer: 2-4        [1, 8]                    8\n",
       "│    └─Linear: 2-5                       [1, 8]                    72\n",
       "│    └─CustomActivationLayer: 2-6        [1, 8]                    8\n",
       "│    └─Linear: 2-7                       [1, 8]                    72\n",
       "│    └─CustomActivationLayer: 2-8        [1, 8]                    8\n",
       "│    └─Linear: 2-9                       [1, 1]                    9\n",
       "==========================================================================================\n",
       "Total params: 273\n",
       "Trainable params: 273\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedFowardPRePU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedFowardPRePU, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            CustomActivationLayer(8),\n",
    "            nn.Linear(8, 8),\n",
    "            CustomActivationLayer(8),\n",
    "            nn.Linear(8, 8),\n",
    "            CustomActivationLayer(8),\n",
    "            nn.Linear(8, 8),\n",
    "            CustomActivationLayer(8),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "model_prepu = FeedFowardPRePU()\n",
    "summary(model_prepu, input_size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, features, labels, batch_size=1, validation_size=0.0, shuffle=False):\n",
    "\n",
    "        if validation_size > 0:\n",
    "            train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "                features, labels, test_size=validation_size, random_state=42\n",
    "            )\n",
    "            self.train_loader = DataLoader(\n",
    "                TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels).float()),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "            )\n",
    "            self.val_loader = DataLoader(\n",
    "                TensorDataset(torch.tensor(val_data).float(), torch.tensor(val_labels).float()), batch_size=batch_size, shuffle=shuffle\n",
    "            )\n",
    "        else:\n",
    "            self.train_loader = DataLoader(\n",
    "                TensorDataset(torch.tensor(features).float(), torch.tensor(labels).float()), batch_size=batch_size, shuffle=shuffle\n",
    "            )\n",
    "            self.val_loader = None\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def get_val_loader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "def evaluate_model(model, custom_train_loader, criterion, optimizer):\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in custom_train_loader.get_train_loader():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.view(-1, 1))\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100)\n",
    "            # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=100)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(custom_train_loader.get_train_loader())\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in custom_train_loader.get_val_loader():\n",
    "                outputs = model(inputs.view(-1, 1))\n",
    "                val_loss = criterion(outputs, labels.view(-1, 1))\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(custom_train_loader.get_val_loader())\n",
    "        print(f\"Epoch {epoch+1:4d} | Train Loss: {avg_train_loss:10.4f} | Validation Loss: {avg_val_loss:10.4f}\")\n",
    "\n",
    "custom_train_loader = CustomDataLoader(x_values_normal, y_values_normal, batch_size=6 * 6, validation_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:smaller;\">\n",
    "<div>PRePU activation shows vastly superior ability to approximate polynomial compared to ReLU.</div> \n",
    "<div>This is potentially useful for time series data, which tends to follow polynomial structure, as well as other types of data.</div>\n",
    "\n",
    "<b>Model with ReLU activation:</b><br>\n",
    "Epoch  100 | Train Loss:  2775.5210 | Validation Loss:  2845.0250<br>\n",
    "\n",
    "<b>Model with PRePU activation:</b><br>\n",
    "Epoch  100 | Train Loss:     0.2723 | Validation Loss:     0.2764\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | Train Loss:  9832.7991 | Validation Loss:  8405.0763\n",
      "Epoch    2 | Train Loss:  7785.5992 | Validation Loss:  5194.0731\n",
      "Epoch    3 | Train Loss:  5155.2594 | Validation Loss:  4136.2041\n",
      "Epoch    4 | Train Loss:  4543.8516 | Validation Loss:  3713.4670\n",
      "Epoch    5 | Train Loss:  4899.1888 | Validation Loss:  3199.1072\n",
      "Epoch    6 | Train Loss:  3590.4449 | Validation Loss:  2831.2832\n",
      "Epoch    7 | Train Loss:  3271.0715 | Validation Loss:  2466.1872\n",
      "Epoch    8 | Train Loss:  2847.0595 | Validation Loss:  2172.3354\n",
      "Epoch    9 | Train Loss:  2433.1323 | Validation Loss:  1769.5347\n",
      "Epoch   10 | Train Loss:  2129.7823 | Validation Loss:  1398.4995\n",
      "Epoch   11 | Train Loss:  1824.2780 | Validation Loss:  1229.6812\n",
      "Epoch   12 | Train Loss:  1587.8751 | Validation Loss:  1004.9767\n",
      "Epoch   13 | Train Loss:  1369.7085 | Validation Loss:   854.8282\n",
      "Epoch   14 | Train Loss:  1174.7227 | Validation Loss:   738.9935\n",
      "Epoch   15 | Train Loss:  1048.0580 | Validation Loss:   590.1040\n",
      "Epoch   16 | Train Loss:   912.3412 | Validation Loss:   523.3829\n",
      "Epoch   17 | Train Loss:   801.4296 | Validation Loss:   445.8535\n",
      "Epoch   18 | Train Loss:   716.8648 | Validation Loss:   370.4890\n",
      "Epoch   19 | Train Loss:   637.2474 | Validation Loss:   343.1499\n",
      "Epoch   20 | Train Loss:   579.0354 | Validation Loss:   317.2710\n",
      "Epoch   21 | Train Loss:   537.7126 | Validation Loss:   293.1893\n",
      "Epoch   22 | Train Loss:   482.2427 | Validation Loss:   239.9524\n",
      "Epoch   23 | Train Loss:   442.5292 | Validation Loss:   203.1028\n",
      "Epoch   24 | Train Loss:   410.6821 | Validation Loss:   183.5056\n",
      "Epoch   25 | Train Loss:   377.9662 | Validation Loss:   165.8996\n",
      "Epoch   26 | Train Loss:   343.7872 | Validation Loss:   150.9907\n",
      "Epoch   27 | Train Loss:   313.9437 | Validation Loss:   134.1249\n",
      "Epoch   28 | Train Loss:   300.0328 | Validation Loss:   121.4754\n",
      "Epoch   29 | Train Loss:   281.8130 | Validation Loss:   103.8529\n",
      "Epoch   30 | Train Loss:   259.7450 | Validation Loss:    95.3828\n",
      "Epoch   31 | Train Loss:   237.7108 | Validation Loss:    95.3405\n",
      "Epoch   32 | Train Loss:   226.9141 | Validation Loss:    77.2426\n",
      "Epoch   33 | Train Loss:   212.6279 | Validation Loss:    85.2219\n",
      "Epoch   34 | Train Loss:   201.9456 | Validation Loss:    76.3125\n",
      "Epoch   35 | Train Loss:   188.4568 | Validation Loss:    70.0703\n",
      "Epoch   36 | Train Loss:   183.1972 | Validation Loss:    63.3617\n",
      "Epoch   37 | Train Loss:   174.5103 | Validation Loss:    58.1796\n",
      "Epoch   38 | Train Loss:   170.3340 | Validation Loss:    55.9496\n",
      "Epoch   39 | Train Loss:   159.6313 | Validation Loss:    46.2601\n",
      "Epoch   40 | Train Loss:   159.2091 | Validation Loss:    64.1027\n",
      "Epoch   41 | Train Loss:   147.8857 | Validation Loss:    54.5460\n",
      "Epoch   42 | Train Loss:   142.5003 | Validation Loss:    50.9825\n",
      "Epoch   43 | Train Loss:   140.1029 | Validation Loss:    45.7868\n",
      "Epoch   44 | Train Loss:   134.4956 | Validation Loss:    43.6627\n",
      "Epoch   45 | Train Loss:   133.2562 | Validation Loss:    34.9536\n",
      "Epoch   46 | Train Loss:   127.0201 | Validation Loss:    42.8212\n",
      "Epoch   47 | Train Loss:   129.4441 | Validation Loss:    34.7876\n",
      "Epoch   48 | Train Loss:   122.5131 | Validation Loss:    33.7047\n",
      "Epoch   49 | Train Loss:   120.0664 | Validation Loss:    35.6390\n",
      "Epoch   50 | Train Loss:   117.1058 | Validation Loss:    35.4600\n",
      "Epoch   51 | Train Loss:   114.5105 | Validation Loss:    29.7263\n",
      "Epoch   52 | Train Loss:   105.8464 | Validation Loss:    30.1403\n",
      "Epoch   53 | Train Loss:   109.3324 | Validation Loss:    32.4022\n",
      "Epoch   54 | Train Loss:   104.1900 | Validation Loss:    29.4713\n",
      "Epoch   55 | Train Loss:    99.1116 | Validation Loss:    41.9807\n",
      "Epoch   56 | Train Loss:    98.4963 | Validation Loss:    25.0649\n",
      "Epoch   57 | Train Loss:    96.2675 | Validation Loss:    23.5436\n",
      "Epoch   58 | Train Loss:    92.8124 | Validation Loss:    22.9585\n",
      "Epoch   59 | Train Loss:    91.5387 | Validation Loss:    21.7405\n",
      "Epoch   60 | Train Loss:    86.2414 | Validation Loss:    21.9588\n",
      "Epoch   61 | Train Loss:    84.5277 | Validation Loss:    27.3668\n",
      "Epoch   62 | Train Loss:    83.4022 | Validation Loss:    21.9913\n",
      "Epoch   63 | Train Loss:    85.0466 | Validation Loss:    20.1470\n",
      "Epoch   64 | Train Loss:    78.0929 | Validation Loss:    19.3763\n",
      "Epoch   65 | Train Loss:    78.1847 | Validation Loss:    21.6075\n",
      "Epoch   66 | Train Loss:    73.7103 | Validation Loss:    23.3041\n",
      "Epoch   67 | Train Loss:    76.1745 | Validation Loss:    16.8655\n",
      "Epoch   68 | Train Loss:    70.7891 | Validation Loss:    16.0123\n",
      "Epoch   69 | Train Loss:    72.8439 | Validation Loss:    17.2970\n",
      "Epoch   70 | Train Loss:    70.9656 | Validation Loss:    16.4837\n",
      "Epoch   71 | Train Loss:    69.6024 | Validation Loss:    19.6598\n",
      "Epoch   72 | Train Loss:    71.3589 | Validation Loss:    15.0433\n",
      "Epoch   73 | Train Loss:    66.1797 | Validation Loss:    15.6315\n",
      "Epoch   74 | Train Loss:    68.9578 | Validation Loss:    14.1901\n",
      "Epoch   75 | Train Loss:    67.7787 | Validation Loss:    17.8850\n",
      "Epoch   76 | Train Loss:    64.7595 | Validation Loss:    13.6158\n",
      "Epoch   77 | Train Loss:    66.5280 | Validation Loss:    17.5143\n",
      "Epoch   78 | Train Loss:    59.9667 | Validation Loss:    12.3558\n",
      "Epoch   79 | Train Loss:    63.4425 | Validation Loss:    14.4366\n",
      "Epoch   80 | Train Loss:    61.7187 | Validation Loss:    14.6319\n",
      "Epoch   81 | Train Loss:    58.8943 | Validation Loss:    12.4751\n",
      "Epoch   82 | Train Loss:    58.5904 | Validation Loss:    13.2689\n",
      "Epoch   83 | Train Loss:    60.5717 | Validation Loss:    11.8772\n",
      "Epoch   84 | Train Loss:    54.5424 | Validation Loss:    12.3360\n",
      "Epoch   85 | Train Loss:    57.1268 | Validation Loss:    13.1545\n",
      "Epoch   86 | Train Loss:    58.9156 | Validation Loss:    12.4167\n",
      "Epoch   87 | Train Loss:    58.7061 | Validation Loss:    13.4536\n",
      "Epoch   88 | Train Loss:    56.6133 | Validation Loss:    11.2912\n",
      "Epoch   89 | Train Loss:    57.1570 | Validation Loss:    12.9031\n",
      "Epoch   90 | Train Loss:    53.3375 | Validation Loss:    13.7298\n",
      "Epoch   91 | Train Loss:    54.2783 | Validation Loss:    11.7099\n",
      "Epoch   92 | Train Loss:    52.2703 | Validation Loss:    16.9581\n",
      "Epoch   93 | Train Loss:    50.9169 | Validation Loss:    10.8535\n",
      "Epoch   94 | Train Loss:    49.8727 | Validation Loss:    12.1559\n",
      "Epoch   95 | Train Loss:    50.1485 | Validation Loss:    13.4895\n",
      "Epoch   96 | Train Loss:    46.4743 | Validation Loss:    10.4451\n",
      "Epoch   97 | Train Loss:    44.4384 | Validation Loss:    12.1614\n",
      "Epoch   98 | Train Loss:    76.7557 | Validation Loss:    10.1210\n",
      "Epoch   99 | Train Loss:    48.6546 | Validation Loss:    10.3496\n",
      "Epoch  100 | Train Loss:    47.0137 | Validation Loss:    10.3185\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_relu.parameters())\n",
    "evaluate_model(model_relu, custom_train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | Train Loss:  9947.8263 | Validation Loss:  8573.0011\n",
      "Epoch    2 | Train Loss:  4872.8469 | Validation Loss:  3786.1320\n",
      "Epoch    3 | Train Loss:  3302.2997 | Validation Loss:  1102.5246\n",
      "Epoch    4 | Train Loss:   474.7575 | Validation Loss:   232.0756\n",
      "Epoch    5 | Train Loss:   163.7916 | Validation Loss:     5.5412\n",
      "Epoch    6 | Train Loss:     9.3231 | Validation Loss:     4.7504\n",
      "Epoch    7 | Train Loss:     7.7992 | Validation Loss:     2.7427\n",
      "Epoch    8 | Train Loss:     5.7084 | Validation Loss:     2.1191\n",
      "Epoch    9 | Train Loss:     3.9655 | Validation Loss:     1.2975\n",
      "Epoch   10 | Train Loss:     4.5550 | Validation Loss:     1.1546\n",
      "Epoch   11 | Train Loss:     2.8098 | Validation Loss:     3.7431\n",
      "Epoch   12 | Train Loss:     3.2621 | Validation Loss:     1.2849\n",
      "Epoch   13 | Train Loss:     3.8715 | Validation Loss:     3.5378\n",
      "Epoch   14 | Train Loss:     1.5759 | Validation Loss:     1.1625\n",
      "Epoch   15 | Train Loss:     1.9998 | Validation Loss:     0.3459\n",
      "Epoch   16 | Train Loss:     1.4479 | Validation Loss:     0.5035\n",
      "Epoch   17 | Train Loss:     1.3108 | Validation Loss:     0.4046\n",
      "Epoch   18 | Train Loss:     1.0306 | Validation Loss:     0.5688\n",
      "Epoch   19 | Train Loss:     0.8316 | Validation Loss:     0.2770\n",
      "Epoch   20 | Train Loss:     1.1914 | Validation Loss:     0.1198\n",
      "Epoch   21 | Train Loss:     0.6144 | Validation Loss:     0.1337\n",
      "Epoch   22 | Train Loss:     0.8947 | Validation Loss:     0.3023\n",
      "Epoch   23 | Train Loss:     0.5900 | Validation Loss:     1.3197\n",
      "Epoch   24 | Train Loss:     0.7078 | Validation Loss:     0.4119\n",
      "Epoch   25 | Train Loss:     0.7047 | Validation Loss:     0.2447\n",
      "Epoch   26 | Train Loss:     0.5870 | Validation Loss:     0.2134\n",
      "Epoch   27 | Train Loss:     0.4302 | Validation Loss:     0.3130\n",
      "Epoch   28 | Train Loss:     0.4884 | Validation Loss:     0.0546\n",
      "Epoch   29 | Train Loss:     0.3707 | Validation Loss:     0.0522\n",
      "Epoch   30 | Train Loss:     0.5431 | Validation Loss:     0.3436\n",
      "Epoch   31 | Train Loss:     0.5139 | Validation Loss:     0.1820\n",
      "Epoch   32 | Train Loss:     0.6474 | Validation Loss:     0.5775\n",
      "Epoch   33 | Train Loss:     0.8323 | Validation Loss:     0.2466\n",
      "Epoch   34 | Train Loss:     0.4650 | Validation Loss:     0.0787\n",
      "Epoch   35 | Train Loss:     0.4939 | Validation Loss:     0.2349\n",
      "Epoch   36 | Train Loss:     0.4389 | Validation Loss:     0.4159\n",
      "Epoch   37 | Train Loss:     0.3580 | Validation Loss:     0.2213\n",
      "Epoch   38 | Train Loss:     0.2904 | Validation Loss:     0.1050\n",
      "Epoch   39 | Train Loss:     0.4874 | Validation Loss:     0.5511\n",
      "Epoch   40 | Train Loss:     0.6595 | Validation Loss:     0.1315\n",
      "Epoch   41 | Train Loss:     0.4171 | Validation Loss:     0.1262\n",
      "Epoch   42 | Train Loss:     0.2663 | Validation Loss:     0.0892\n",
      "Epoch   43 | Train Loss:     0.3192 | Validation Loss:     0.6706\n",
      "Epoch   44 | Train Loss:     0.4319 | Validation Loss:     1.0808\n",
      "Epoch   45 | Train Loss:     0.3354 | Validation Loss:     0.1035\n",
      "Epoch   46 | Train Loss:     0.2553 | Validation Loss:     0.2357\n",
      "Epoch   47 | Train Loss:     0.1933 | Validation Loss:     0.0445\n",
      "Epoch   48 | Train Loss:     0.4508 | Validation Loss:     0.3314\n",
      "Epoch   49 | Train Loss:     0.2111 | Validation Loss:     0.2629\n",
      "Epoch   50 | Train Loss:     0.4847 | Validation Loss:     1.1303\n",
      "Epoch   51 | Train Loss:     0.1915 | Validation Loss:     0.0139\n",
      "Epoch   52 | Train Loss:     0.3346 | Validation Loss:     0.0829\n",
      "Epoch   53 | Train Loss:     0.2299 | Validation Loss:     0.4796\n",
      "Epoch   54 | Train Loss:     0.4767 | Validation Loss:     0.0409\n",
      "Epoch   55 | Train Loss:     0.3728 | Validation Loss:     0.9611\n",
      "Epoch   56 | Train Loss:     0.3421 | Validation Loss:     0.0154\n",
      "Epoch   57 | Train Loss:     0.3663 | Validation Loss:     0.2793\n",
      "Epoch   58 | Train Loss:     0.2140 | Validation Loss:     0.2085\n",
      "Epoch   59 | Train Loss:     0.3128 | Validation Loss:     0.2212\n",
      "Epoch   60 | Train Loss:     0.4106 | Validation Loss:     0.9192\n",
      "Epoch   61 | Train Loss:     0.1766 | Validation Loss:     0.3451\n",
      "Epoch   62 | Train Loss:     0.2599 | Validation Loss:     0.6139\n",
      "Epoch   63 | Train Loss:     0.3153 | Validation Loss:     0.6948\n",
      "Epoch   64 | Train Loss:     0.2681 | Validation Loss:     0.0533\n",
      "Epoch   65 | Train Loss:     0.4285 | Validation Loss:     0.5237\n",
      "Epoch   66 | Train Loss:     0.1932 | Validation Loss:     0.2500\n",
      "Epoch   67 | Train Loss:     0.2181 | Validation Loss:     0.4166\n",
      "Epoch   68 | Train Loss:     0.2700 | Validation Loss:     0.8219\n",
      "Epoch   69 | Train Loss:     0.4655 | Validation Loss:     0.0478\n",
      "Epoch   70 | Train Loss:     0.2370 | Validation Loss:     0.1633\n",
      "Epoch   71 | Train Loss:     0.2120 | Validation Loss:     0.0281\n",
      "Epoch   72 | Train Loss:     0.1892 | Validation Loss:     1.0452\n",
      "Epoch   73 | Train Loss:     0.4418 | Validation Loss:     0.0299\n",
      "Epoch   74 | Train Loss:     0.2275 | Validation Loss:     0.0638\n",
      "Epoch   75 | Train Loss:     0.2743 | Validation Loss:     0.1086\n",
      "Epoch   76 | Train Loss:     0.2968 | Validation Loss:     0.0842\n",
      "Epoch   77 | Train Loss:     0.4599 | Validation Loss:     0.0615\n",
      "Epoch   78 | Train Loss:     0.2655 | Validation Loss:     0.2610\n",
      "Epoch   79 | Train Loss:     0.1655 | Validation Loss:     0.0220\n",
      "Epoch   80 | Train Loss:     0.2032 | Validation Loss:     0.4098\n",
      "Epoch   81 | Train Loss:     0.3247 | Validation Loss:     0.0438\n",
      "Epoch   82 | Train Loss:     0.2433 | Validation Loss:     0.0129\n",
      "Epoch   83 | Train Loss:     0.3794 | Validation Loss:     0.3819\n",
      "Epoch   84 | Train Loss:     0.2388 | Validation Loss:     0.1275\n",
      "Epoch   85 | Train Loss:     0.3258 | Validation Loss:     0.3536\n",
      "Epoch   86 | Train Loss:     0.1858 | Validation Loss:     0.0102\n",
      "Epoch   87 | Train Loss:     0.1826 | Validation Loss:     0.0712\n",
      "Epoch   88 | Train Loss:     0.1547 | Validation Loss:     0.2531\n",
      "Epoch   89 | Train Loss:     0.2375 | Validation Loss:     0.4452\n",
      "Epoch   90 | Train Loss:     0.2777 | Validation Loss:     0.0561\n",
      "Epoch   91 | Train Loss:     0.2102 | Validation Loss:     0.0226\n",
      "Epoch   92 | Train Loss:     0.2933 | Validation Loss:     0.0644\n",
      "Epoch   93 | Train Loss:     0.2819 | Validation Loss:     0.2498\n",
      "Epoch   94 | Train Loss:     0.2867 | Validation Loss:     0.3554\n",
      "Epoch   95 | Train Loss:     0.1739 | Validation Loss:     0.0572\n",
      "Epoch   96 | Train Loss:     0.2167 | Validation Loss:     0.4368\n",
      "Epoch   97 | Train Loss:     0.4112 | Validation Loss:     0.1525\n",
      "Epoch   98 | Train Loss:     0.2378 | Validation Loss:     0.5779\n",
      "Epoch   99 | Train Loss:     0.2838 | Validation Loss:     0.3845\n",
      "Epoch  100 | Train Loss:     0.3361 | Validation Loss:     0.1564\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_prepu.parameters())\n",
    "evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for layers.1.scale: tensor([0.4539, 0.0359, 0.0324, 0.5567, 0.4577, 0.4319, 0.8211, 0.0226])\n",
      "Power for layers.1.scale: tensor([1.3122, 1.0346, 1.0314, 1.3576, 1.3140, 1.3016, 1.4509, 1.0221])\n",
      "\n",
      "Weights for layers.3.scale: tensor([0.0088, 0.6225, 0.0000, 0.0000, 0.4588, 0.3729, 0.0000, 0.4976])\n",
      "Power for layers.3.scale: tensor([1.0088, 1.3836, 1.0000, 1.0000, 1.3145, 1.2716, 1.0000, 1.3323])\n",
      "\n",
      "Weights for layers.5.scale: tensor([ 0.3419,  0.0000,  0.3977, -0.8998,  0.4199,  0.0409,  0.2212,  0.4863])\n",
      "Power for layers.5.scale: tensor([1.2548, 1.0000, 1.2846, 0.5264, 1.2957, 1.0393, 1.1812, 1.3272])\n",
      "\n",
      "Weights for layers.7.scale: tensor([-0.3667,  0.3235, -0.1554,  0.5459,  0.4005,  0.0455,  0.3236, -0.6362])\n",
      "Power for layers.7.scale: tensor([0.7317, 1.2444, 0.8655, 1.3531, 1.2859, 1.0436, 1.2445, 0.6112])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_prepu.named_parameters():\n",
    "    if 'scale' in name.lower():\n",
    "        print(f\"Weights for {name}: {param.data}\")\n",
    "        print(f\"Power for {name}: {F.softsign(param.data) + 1}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Read IEEE Improved Polynomial Neural Networks with Normalised Activations (can help give an idea on gradient instablity)\n",
    "# Look up polynomial activation function\n",
    "# Test on more realistic dataset\n",
    "# Look up rectified power unit\n",
    "# Read Why Rectified Power (RePU) Activation Functions Are Efficient in Deep Learning: A Theoretical Explanation\n",
    "# Do more research on polynomial activation function or power activation function\n",
    "# Read https://www.researchgate.net/publication/330295973_Piecewise_Polynomial_Activation_Functions_for_Feedforward_Neural_Networks\n",
    "# Consensus ChatGPT \"research on polynomial activation function or power activation function\"\n",
    "# http://sigtbd.csail.mit.edu/pubs/2018/sigtbd18-paper-4.pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
